{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex11_1 Inception Model\n",
    "本模型是一个简化版的GoogleNet的实现，同样针对MNIST数据集的卷积神经网络\n",
    "主要是封装一个Inception模型， 此模型的直观思想在训练过程中对不同架构的神经网络分支进行一个自动的参数选择，从而选出最优的分支\n",
    "此模型比上一个两层卷积的神经网络性能更优，经过10轮epoch,在测试集上准确率可达98.9%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 初始化基本设置并建立数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current computing device is cpu \n",
      "[size of train_set/test_set]:60000,10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 初始化并固定随机种子\n",
    "\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "setup_seed(1012)\n",
    "\n",
    "# 设置GPU加速\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"The current computing device is {device.type} \")\n",
    "if torch.cuda.is_available():\n",
    "    print(f'The current GPU is :{torch.cuda.get_device_name(0)}')\n",
    "\n",
    "# prepare dataset\n",
    "\n",
    "batch_size = 64\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307,), (0.3081,))])\n",
    "# 处理图像数据的一个转换类 将pillow类转化为tensor, 并将值归一化： 0.1307 和 0.3081 为该数据集的均值和标准差\n",
    "# 每一个数据为[28,28]的tensor\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='./dataset/mnist/', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_dataset = datasets.MNIST(\n",
    "    root='./dataset/mnist/', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False,\n",
    "                         batch_size=len(test_dataset))  # 测试肯定是\n",
    "\n",
    "\n",
    "print(f'[size of train_set/test_set]:{len(train_dataset)},{len(test_dataset)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (layers): Sequential(\n",
       "    (0): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU()\n",
       "    (3): Inception(\n",
       "      (branch_pool): Sequential(\n",
       "        (0): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "        (1): Conv2d(10, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (branch1x1): Conv2d(10, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (branch5x5): Sequential(\n",
       "        (0): Conv2d(10, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(16, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      )\n",
       "      (branch3x3): Sequential(\n",
       "        (0): Conv2d(10, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(16, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (4): Conv2d(88, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): ReLU()\n",
       "    (7): Inception(\n",
       "      (branch_pool): Sequential(\n",
       "        (0): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
       "        (1): Conv2d(20, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (branch1x1): Conv2d(20, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (branch5x5): Sequential(\n",
       "        (0): Conv2d(20, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(16, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      )\n",
       "      (branch3x3): Sequential(\n",
       "        (0): Conv2d(20, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(16, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=1408, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Inception(nn.Module):\n",
    "    '''\n",
    "    input [batch_size, channels, width, height]:b,c,w,h\n",
    "\n",
    "    初始化时需要指定input_channels\n",
    "\n",
    "    此模型的输出channel数目为 88 = 24+16+24+24, concatenate four branches with dim 1(channels)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_channels) -> None:\n",
    "        super(Inception,self).__init__()\n",
    "        self.branch_pool = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels, 24, kernel_size=1)\n",
    "        )\n",
    "        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.branch5x5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=1),\n",
    "            nn.Conv2d(16, 24, kernel_size=5, padding=2)\n",
    "        )\n",
    "        self.branch3x3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=1),\n",
    "            nn.Conv2d(16, 24, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(24, 24, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch_pool = self.branch_pool(x)\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "        branch5x5 = self.branch5x5(x)\n",
    "        branch3x3 = self.branch3x3(x)\n",
    "        output = [branch_pool, branch1x1, branch5x5, branch3x3]\n",
    "        return torch.cat(output, dim=1)  # [b,c,w,h] c = 88\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Net,self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            # 输入 [b,1,28,28]\n",
    "            nn.Conv2d(1, 10, kernel_size=5),  # [b,10,24,24]\n",
    "            nn.MaxPool2d(2),  # [b,10,12,12]\n",
    "            nn.ReLU(),\n",
    "            Inception(10),  # [b,88,12,12]\n",
    "            nn.Conv2d(88, 20, kernel_size=5),  # [b,20,8,8]\n",
    "            nn.MaxPool2d(2),    # [b,20,4,4]\n",
    "            nn.ReLU(),\n",
    "            Inception(20)   # [b,88,4,4] (b*1408)\n",
    "        )\n",
    "        self.fc = nn.Linear(1408, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.layers(x)\n",
    "        x = x.view(batch_size,-1)   #[b,1408]\n",
    "        return self.fc(x)   #[b,10]\n",
    "    \n",
    "model = Net()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设置损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct loss and optimiter\n",
    "\n",
    "# 包含了softmax层，并且会根据标签类别（即使是多类）,自动构建one-hot计算交叉熵，需要LongTensor类标签\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设置训练和测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and test\n",
    "\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    '''某一轮epoch上的训练'''\n",
    "    epoch_loss = []  # 记录该轮epoch上每个batch的loss\n",
    "    for batch_idx, batch_data in enumerate(train_loader, 1):\n",
    "        X, y_label = batch_data\n",
    "        X, y_label = X.to(device), y_label.to(device)\n",
    "        # print(\"debug here: X shape:\", X.shape)\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y_label)\n",
    "\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    average_loss = sum(epoch_loss)/len(epoch_loss)\n",
    "    loss_list.append(average_loss)\n",
    "    print(f'[epoch]:{epoch},  [average_loss]: {average_loss}')\n",
    "\n",
    "\n",
    "def test():\n",
    "    '''在全集合上测试一次准确率'''\n",
    "    correct_num = 0\n",
    "    num = len(test_dataset)\n",
    "    with torch.no_grad():\n",
    "        for batch_data in test_loader:\n",
    "            X, y = batch_data\n",
    "            X, y = X.to(device) ,y.to(device)\n",
    "            y_pred = model(X)\n",
    "            y_pred = torch.argmax(y_pred, dim=1)\n",
    "            correct_num += torch.sum(y_pred == y).item()\n",
    "    accuracy = correct_num/num\n",
    "    accuracy_list.append(accuracy)\n",
    "    print(f'Current accuracy on the test set is {accuracy}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练与测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch]:1,  [average_loss]: 0.369774169503038\n",
      "Current accuracy on the whole set is 0.9661\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\WorkAndStudy\\PyTorch深度学习实践\\exercise\\ex_11_1.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/WorkAndStudy/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/exercise/ex_11_1.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/WorkAndStudy/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/exercise/ex_11_1.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/WorkAndStudy/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/exercise/ex_11_1.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     train(epoch)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/WorkAndStudy/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/exercise/ex_11_1.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     test()\n",
      "\u001b[1;32md:\\WorkAndStudy\\PyTorch深度学习实践\\exercise\\ex_11_1.ipynb Cell 11\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/WorkAndStudy/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/exercise/ex_11_1.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m'''某一轮epoch上的训练'''\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/WorkAndStudy/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/exercise/ex_11_1.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m epoch_loss \u001b[39m=\u001b[39m []  \u001b[39m# 记录该轮epoch上每个batch的loss\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/WorkAndStudy/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/exercise/ex_11_1.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, batch_data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader, \u001b[39m1\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/WorkAndStudy/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/exercise/ex_11_1.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     X, y_label \u001b[39m=\u001b[39m batch_data\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/WorkAndStudy/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/exercise/ex_11_1.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     X, y_label \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), y_label\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32md:\\soft\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\soft\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\soft\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\soft\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\soft\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32md:\\soft\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\transforms\\transforms.py:94\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     93\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 94\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     95\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32md:\\soft\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\transforms\\transforms.py:134\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    127\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[1;32md:\\soft\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\transforms\\functional.py:168\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    167\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n\u001b[1;32m--> 168\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mview(pic\u001b[39m.\u001b[39;49msize[\u001b[39m1\u001b[39;49m], pic\u001b[39m.\u001b[39;49msize[\u001b[39m0\u001b[39;49m], \u001b[39mlen\u001b[39;49m(pic\u001b[39m.\u001b[39;49mgetbands()))\n\u001b[0;32m    169\u001b[0m \u001b[39m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    170\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mpermute((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start training now!\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11d37945a31fbc9f387fe3fa9b4dd08cf0c3fbd61804a4f367c7e90d53f74884"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
