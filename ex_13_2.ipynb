{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ex_13_2 RNN\n",
    "\n",
    "根据名字识别他所在的国家\n",
    "\n",
    "人名字符长短不一，最长的10个字符，所以处理成10维输入张量，都是英文字母刚好可以映射到ASCII上\n",
    "\n",
    "Maclean ->  ['M', 'a', 'c', 'l', 'e', 'a', 'n'] ->  [ 77 97 99 108 101 97 110]  ->  [ 77 97 99 108 101 97 110 0 0 0]\n",
    "\n",
    "共有18个国家，设置索引为0-17\n",
    "\n",
    "训练集和测试集的表格文件都是第一列人名，第二列国家"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current computing device is cpu \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import csv\n",
    "import gzip\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Parameters\n",
    "HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 256\n",
    "N_LAYER = 2\n",
    "N_EPOCHS = 100\n",
    "N_CHARS = 128\n",
    "USE_GPU = True\n",
    "\n",
    "# 初始化并固定随机种子\n",
    "\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "setup_seed(1012)\n",
    "\n",
    "# 设置GPU加速\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"The current computing device is {device.type} \")\n",
    "if torch.cuda.is_available():\n",
    "    print(f'The current GPU is :{torch.cuda.get_device_name(0)}')\n",
    "\n",
    "\n",
    "def create_tensor(tensor):  # 如果使用GPU，把tensor搬到GPU上去\n",
    "    tensor = tensor.to(device)\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameDataset(Dataset):  # 处理数据集\n",
    "    def __init__(self, is_train_set=True):\n",
    "        filename = 'names_train.csv.gz' if is_train_set else 'names_test.csv.gz'\n",
    "        with gzip.open(filename, 'rt') as f:  # 打开压缩文件并将变量名设为为f\n",
    "            reader = csv.reader(f)  # 读取表格文件\n",
    "            rows = list(reader)\n",
    "        self.names = [row[0] for row in rows]               # 所有数据集中的姓名\n",
    "        self.countries = [row[1] for row in rows]           # 所有数据集中的国家\n",
    "        self.len = len(self.names)                      # 数据集总长度\n",
    "        # 保存所有国家名（各一次） 索引与值正好与词典对偶\n",
    "        self.country_list = list(set(self.countries))\n",
    "        self.country_num = len(self.country_list)   # 所有国家的类别总数\n",
    "        self.country_dict = self.getCountryDict()   # 返回一个词典，键：国家名 值：数字\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.names[index], self.country_dict[self.countries[index]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def getCountryDict(self):\n",
    "        country_dict = dict()  # 创建空字典\n",
    "        for idx, country in enumerate(self.country_list, 0):\n",
    "            country_dict[country] = idx  # 或者 len(country_dict)\n",
    "        return country_dict\n",
    "\n",
    "    def idx2country(self, index):  # 由国家代表的值返回对应的国家名\n",
    "        return self.country_list[index]\n",
    "\n",
    "    def getCountrysNum(self):  # 返回国家类别数量\n",
    "        return self.country_num\n",
    "\n",
    "\n",
    "train_set = NameDataset(is_train_set=True)\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_set = NameDataset(is_train_set=False)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "N_COUNTRY = train_set.getCountrysNum()  # 这也是最终输出的维度(每个预测类别对应一个维度)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNBaseClassifier(\n",
       "  (embedding): Embedding(128, 100)\n",
       "  (gru): GRU(100, 100, num_layers=2, bidirectional=True)\n",
       "  (fc): Linear(in_features=200, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 不使用 pack_padded_sequence 方法\n",
    "class RNNBaseClassifier(nn.Module): # 基于RNN的分类器（最后会连上一个全连接层）\n",
    "    def __init__(self,input_size,hidden_size,output_size,num_layers=1,bidirectional= True) -> None:\n",
    "        super(RNNBaseClassifier,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.n_directions = 2 if bidirectional else 1\n",
    "        \n",
    "        # 定义模型与参数\n",
    "        self.embedding = nn.Embedding(input_size,hidden_size) # 注意需要LongTensor!\n",
    "        self.gru = nn.GRU(input_size = self.hidden_size,hidden_size = self.hidden_size,num_layers = self.num_layers,bidirectional=bidirectional)\n",
    "        # 输入 [input, h_0]，注意隐藏层初始状态h0需要自己给定，那就写一个_init_hidden函数\n",
    "        # input 维度要求 [seq_length,batch_size,input_size] (默认batch_first = False)\n",
    "        # 输出output是中间每次输出，最终结果h_n维度为[bidirectional*num_layers,batch_size,hidden_size]\n",
    "        self.fc = nn.Linear(self.hidden_size*self.n_directions,output_size)\n",
    "        # 把RNN最后一层的最后的输出作为RNN的输入，其应该具有维度[batch_size,hidden_size * n_directions]\n",
    "        # 即如果是双向的RNN，最后一次输出需要拼接两个隐层，对应维度乘2\n",
    "    \n",
    "    def _init_hidden(self,batch_size):\n",
    "        return create_tensor(torch.zeros(self.n_directions*self.num_layers,batch_size,self.hidden_size))\n",
    "    \n",
    "    def forward(self,input,seq_lengths = None):\n",
    "        # 此模块的input维度为[batch_size, seq_length]\n",
    "        batch_size = input.shape[0] # 初始隐层\n",
    "        input = input.t()   # [seq_length,batch_size]\n",
    "        h0 = self._init_hidden(batch_size)\n",
    "        gru_input = self.embedding(input)   # [seq_length,batch_size,input_size(即hidden_size)]\n",
    "        _,hidden = self.gru(gru_input,h0)   # [bidirectional*num_layers,batch_size,hidden_size]\n",
    "        if self.n_directions == 2:\n",
    "            hidden_cat = torch.cat([hidden[-1],hidden[-2]],dim=1) #最后一层RNN 的 最后两个隐层状态\n",
    "        else:\n",
    "            hidden_cat = hidden[-1] # 单向RNN，最后一层的最终隐层状态\n",
    "        return self.fc(hidden_cat)\n",
    "        \n",
    "\n",
    "\n",
    "def name2list(name):\n",
    "    \"\"\"输入一个姓名字符串，返回ASCII码表示的姓名列表 与 列表长度\"\"\"\n",
    "    arr = [ord(c) for c in name]\n",
    "    return arr, len(arr)\n",
    "\n",
    "def make_tensor(names,country):\n",
    "    '''\n",
    "    接收Dataloader每次迭代的数据[batch_size,name(string)] 和 [batch_size,country(id)]\n",
    "    \n",
    "    返回可传入RNNClassifier的数据: 将每一个name(string)转化为一个长度为seq_length的向量,组成一个矩阵\n",
    "    '''\n",
    "    array = [name2list(name) for name in names]\n",
    "    name_array = [s[0] for s in array]\n",
    "    name_length = [s[1] for s in array]\n",
    "    max_namelength = max(name_length)\n",
    "    country = country.long()\n",
    "    batch_size = len(name_length)\n",
    "    \n",
    "    seq_tensor = torch.zeros(batch_size,max_namelength).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(name_array, name_length), 0):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "    \n",
    "    return create_tensor(seq_tensor),create_tensor(country)\n",
    "\n",
    "model = RNNBaseClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRY, N_LAYER)\n",
    "model.to(device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001) \n",
    "loss_list = []\n",
    "accuracy_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    '''某一轮epoch上的训练'''\n",
    "    epoch_loss = []  # 记录该轮epoch上每个batch的loss\n",
    "    for batch_idx, batch_data in enumerate(train_loader, 1):\n",
    "        X, y_label = batch_data\n",
    "        X, y_label = make_tensor(X,y_label)\n",
    "        # print(\"debug here: X shape:\", X.shape)\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y_label)\n",
    "\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    average_loss = sum(epoch_loss)/len(epoch_loss)\n",
    "    loss_list.append(average_loss)\n",
    "    print(f'[epoch]:{epoch},  [average_loss]: {average_loss}')\n",
    "\n",
    "\n",
    "def test():\n",
    "    '''在全集合上测试一次准确率'''\n",
    "    correct_num = 0\n",
    "    num = len(test_set)\n",
    "    with torch.no_grad():\n",
    "        for batch_data in test_loader:\n",
    "            X, y = batch_data\n",
    "            X, y = make_tensor(X,y)\n",
    "            y_pred = model(X)\n",
    "            y_pred = torch.argmax(y_pred, dim=1)\n",
    "            correct_num += torch.sum(y_pred == y).item()\n",
    "    accuracy = correct_num/num\n",
    "    accuracy_list.append(accuracy)\n",
    "    print(f'Current accuracy on the test set is {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch]:1,  [average_loss]: 1.61169855099804\n",
      "Current accuracy on the test set is 0.6270149253731343\n",
      "[epoch]:2,  [average_loss]: 1.1132923657039426\n",
      "Current accuracy on the test set is 0.6947761194029851\n",
      "[epoch]:3,  [average_loss]: 0.8827043333143558\n",
      "Current accuracy on the test set is 0.7556716417910447\n",
      "[epoch]:4,  [average_loss]: 0.7302949957127841\n",
      "Current accuracy on the test set is 0.7785074626865671\n",
      "[epoch]:5,  [average_loss]: 0.6340223466450313\n",
      "Current accuracy on the test set is 0.7944776119402985\n",
      "[epoch]:6,  [average_loss]: 0.5692326769513903\n",
      "Current accuracy on the test set is 0.7986567164179105\n",
      "[epoch]:7,  [average_loss]: 0.5183945674941225\n",
      "Current accuracy on the test set is 0.8137313432835821\n",
      "[epoch]:8,  [average_loss]: 0.4757377263510002\n",
      "Current accuracy on the test set is 0.812089552238806\n",
      "[epoch]:9,  [average_loss]: 0.43304347485866185\n",
      "Current accuracy on the test set is 0.8225373134328359\n",
      "[epoch]:10,  [average_loss]: 0.39662236312650284\n",
      "Current accuracy on the test set is 0.8276119402985075\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11d37945a31fbc9f387fe3fa9b4dd08cf0c3fbd61804a4f367c7e90d53f74884"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
