{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex11_2 Residual Model\n",
    "本模型是一个简化版的残差网络的实现，同样针对MNIST数据集的卷积神经网络\n",
    "主要是封装一个Residual Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 初始化基本设置并建立数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current computing device is cpu \n",
      "[size of train_set/test_set]:60000,10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 初始化并固定随机种子\n",
    "\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "setup_seed(1012)\n",
    "\n",
    "# 设置GPU加速\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"The current computing device is {device.type} \")\n",
    "if torch.cuda.is_available():\n",
    "    print(f'The current GPU is :{torch.cuda.get_device_name(0)}')\n",
    "\n",
    "# prepare dataset\n",
    "\n",
    "batch_size = 64\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307,), (0.3081,))])\n",
    "# 处理图像数据的一个转换类 将pillow类转化为tensor, 并将值归一化： 0.1307 和 0.3081 为该数据集的均值和标准差\n",
    "# 每一个数据为[28,28]的tensor\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='./dataset/mnist/', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_dataset = datasets.MNIST(\n",
    "    root='./dataset/mnist/', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False,\n",
    "                         batch_size=len(test_dataset))  # 测试肯定是\n",
    "\n",
    "\n",
    "print(f'[size of train_set/test_set]:{len(train_dataset)},{len(test_dataset)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (layers): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): ResidualBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (4): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): ResidualBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    '''\n",
    "    内部有等宽的卷积层，此模块保持维度和channel不变，以使残差连接\n",
    "    '''\n",
    "    def __init__(self,channels) -> None:\n",
    "        super(ResidualBlock,self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(channels,channels,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels,channels,kernel_size=3,padding=1)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        y = self.layers(x)\n",
    "        return F.relu(x+y)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Net,self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            # input [b,1,28,28]\n",
    "            nn.Conv2d(1,16,kernel_size=5),  # [b,16,24,24]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), #[b,16,12,12]\n",
    "            ResidualBlock(16), # 维度和channel均不变\n",
    "            nn.Conv2d(16,32,kernel_size=5), # [b,32,8,8]\n",
    "            nn.MaxPool2d(2),    # [b,32,4,4]\n",
    "            ResidualBlock(32)   \n",
    "            # 之后转化为全连接层 [b,512]\n",
    "        )\n",
    "        self.fc = nn.Linear(512,10)\n",
    "    def forward(self,x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.layers(x)\n",
    "        x = x.view(batch_size,-1)\n",
    "        return self.fc(x)\n",
    "model = Net()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设置损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct loss and optimiter\n",
    "\n",
    "# 包含了softmax层，并且会根据标签类别（即使是多类）,自动构建one-hot计算交叉熵，需要LongTensor类标签\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设置训练和测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and test\n",
    "\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    '''某一轮epoch上的训练'''\n",
    "    epoch_loss = []  # 记录该轮epoch上每个batch的loss\n",
    "    for batch_idx, batch_data in enumerate(train_loader, 1):\n",
    "        X, y_label = batch_data\n",
    "        X, y_label = X.to(device), y_label.to(device)\n",
    "        # print(\"debug here: X shape:\", X.shape)\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y_label)\n",
    "\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    average_loss = sum(epoch_loss)/len(epoch_loss)\n",
    "    print(f'[epoch]:{epoch},  [average_loss]: {average_loss}')\n",
    "\n",
    "\n",
    "def test():\n",
    "    '''在全集合上测试一次准确率'''\n",
    "    correct_num = 0\n",
    "    num = len(test_dataset)\n",
    "    with torch.no_grad():\n",
    "        for batch_data in test_loader:\n",
    "            X, y = batch_data\n",
    "            X, y = X.to(device) ,y.to(device)\n",
    "            y_pred = model(X)\n",
    "            y_pred = torch.argmax(y_pred, dim=1)\n",
    "            correct_num += torch.sum(y_pred == y).item()\n",
    "    accuracy = correct_num/num\n",
    "    accuracy_list.append(accuracy)\n",
    "    print(f'Current accuracy on the whole set is {accuracy}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练与测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch]:1,  [average_loss]: 0.25860325733561124\n",
      "Current accuracy on the whole set is 0.976\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\WorkAndStudy\\PyTorch深度学习实践\\exercise\\ex_11_2.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/WorkAndStudy/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/exercise/ex_11_2.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/WorkAndStudy/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/exercise/ex_11_2.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/WorkAndStudy/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/exercise/ex_11_2.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     train(epoch)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/WorkAndStudy/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/exercise/ex_11_2.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     test()\n",
      "\u001b[1;32md:\\WorkAndStudy\\PyTorch深度学习实践\\exercise\\ex_11_2.ipynb Cell 11\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/WorkAndStudy/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/exercise/ex_11_2.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     epoch_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/WorkAndStudy/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/exercise/ex_11_2.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/WorkAndStudy/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/exercise/ex_11_2.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/WorkAndStudy/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/exercise/ex_11_2.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/WorkAndStudy/PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/exercise/ex_11_2.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m average_loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(epoch_loss)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(epoch_loss)\n",
      "File \u001b[1;32md:\\soft\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32md:\\soft\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start training now!\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train(epoch)\n",
    "    test()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 作图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "epochs = list(range(1,num_epochs))\n",
    "plt.plot(epochs,loss_list , color='#e4007f', label=\"Train loss (average training loss over one epoch)\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, accuracy_list, color='#f19ec2', label=\"test accuracy\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11d37945a31fbc9f387fe3fa9b4dd08cf0c3fbd61804a4f367c7e90d53f74884"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
